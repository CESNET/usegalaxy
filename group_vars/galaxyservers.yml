---
# Galaxy
galaxy_manage_systemd: no
galaxy_create_user: true # False by default, as e.g. you might have a 'galaxy' user provided by LDAP or AD.
galaxy_separate_privileges: true # Best practices for security, configuration is owned by 'root' (or a different user) than the processes
galaxy_manage_paths: true # False by default as your administrator might e.g. have root_squash enabled on NFS. Here we can create the directories so it's fine.
galaxy_manage_cleanup: true
galaxy_layout: root-dir
galaxy_root: /srv/galaxy
galaxy_user: {name: "{{ galaxy_user_name }}", shell: /bin/bash}
  # galaxy_commit_id: 1ad49865fbeb03551cf7774dc0c12e5cd27ff797 # release_23.0
galaxy_commit_id: 72070dbd0a6d0f418d0b8f914825ea2b8e6a0a88
galaxy_force_checkout: true
miniconda_prefix: "{{ galaxy_tool_dependency_dir }}/_conda"
miniconda_version: 24.4.0 # 23.5.2 # 23.5.0 # 4.12.0
miniconda_channels: ['conda-forge', 'defaults']

enable_flower: yes
enable_telegraf: yes

# Galaxy Job Configuration
galaxy_job_config:
  runners:
    local_runner:
      load: galaxy.jobs.runners.local:LocalJobRunner
      workers: 4
    pulsar_runner:
      load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
      galaxy_url: "https://{{ rabbitmq_hostname }}"
      amqp_url: "pyamqp://pulsar:{{ rabbitmq_users_password.pulsar }}@{{ rabbitmq_hostname }}:5671/pulsar?ssl=1"
      amqp_acknowledge: true
      amqp_ack_republish_time: 1200
      amqp_consumer_timeout: 2
      amqp_publish_retry: true
      amqp_publish_retry_max_retries: 60
      manager: _default_
    pulsar_gpu_runner:
      load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
      galaxy_url: "https://{{ rabbitmq_hostname }}"
      amqp_url: "pyamqp://pulsar:{{ rabbitmq_users_password.pulsar }}@{{ rabbitmq_hostname }}:5671/pulsar?ssl=1"
      amqp_acknowledge: true
      amqp_ack_republish_time: 1200
      amqp_consumer_timeout: 2
      amqp_publish_retry: true
      amqp_publish_retry_max_retries: 60
      manager: gpu
    # pulsar_runner:
      # load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
      # galaxy_url: "https://{{ inventory_hostname }}"
      # amqp_url: "pyamqp://pulsar:{{ rabbitmq_users_password.galaxy }}@{{ inventory_hostname }}:5671/pulsar?ssl=1"
      # amqp_acknowledge: true
      # amqp_ack_republish_time: 1200
      # amqp_consumer_timeout: 2
      # amqp_publish_retry: true
      # amqp_publish_retry_max_retries: 60
      # manager: _default_
  handling:
    assign: ['db-skip-locked']
  execution:
    default: tpv_dispatcher
    environments:
      local_env:
        runner: local_runner
        tmp_dir: true
      singularity:
        runner: local_runner
        singularity_enabled: true
        env:
        # Ensuring a consistent collation environment is good for reproducibility.
        - name: LC_ALL
          value: C
        # The cache directory holds the docker containers that get converted
        - name: APPTAINER_CACHEDIR
          value: /tmp/singularity
        # Apptainer uses a temporary directory to build the squashfs filesystem
        - name: APPTAINER_TMPDIR
          value: /tmp
      pulsar:
        runner: pulsar_runner
        default_file_action: remote_transfer
        dependency_resolution: remote
        jobs_directory: "/storage/praha5-elixir/home/galaxyeu/pulsar-cz/files/staging"
        persistence_directory: "/storage/praha5-elixir/home/galaxyeu/pulsar-cz/files/persistent"
        remote_metadata: false
        rewrite_parameters: true
        transport: curl
        outputs_to_working_directory: false
        # submit_native_specification: '-l select=1:ncpus=2:mem=8gb:scratch_local=50gb -l walltime=12:00:00 -q galaxyeu@elixir-pbs.elixir-czech.cz' 
        singularity_enabled: true
        singularity_volumes: "$job_directory:rw,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,/cvmfs/data.galaxyproject.org:ro,$SCRATCHDIR"
        ## Following configuration works!
        # singularity_volumes: "$job_directory:rw,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,/cvmfs/data.galaxyproject.org:ro"
        container_resolvers: 
          - type: explicit_singularity
          - type: mulled_singularity
        env:
        # Ensuring a consistent collation environment is good for reproducibility.
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: "$SCRATCHDIR"
        - name: TMP
          value: "$SCRATCHDIR"
        - name: TEMP
          value: "$SCRATCHDIR"
      pulsar_gpu:
        runner: pulsar_gpu_runner
        default_file_action: remote_transfer
        transport: curl
        dependency_resolution: remote
        jobs_directory: "/storage/praha5-elixir/home/galaxyeu/pulsar-cz/files/staging" 
        persistence_directory: "/storage/praha5-elixir/home/galaxyeu/pulsar-cz/files/persistent" 
        remote_metadata: false 
        rewrite_parameters: true
        outputs_to_working_directory: false
        singularity_enabled: true
        singularity_run_extra_arguments: '--nv'
        singularity_volumes: '$job_directory:ro,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,$SCRATCHDIR,$ALPHAFOLD_DB:/data:ro'
        env:
        # The cache directory holds the docker containers that get converted
        - name: SINGULARITY_CACHEDIR
          value: "/storage/praha5-elixir/home/galaxyeu/singularity/cache"
        - name: APPTAINER_CACHEDIR
          value: "/storage/praha5-elixir/home/galaxyeu/singularity/cache"
        # Singularity uses a temporary directory to build the squashfs filesystem
        - name: SINGULARITY_TMPDIR
          value: "/storage/praha5-elixir/home/galaxyeu/singularity/tmp"
        - name: APPTAINER_TMPDIR
          value: "/storage/praha5-elixir/home/galaxyeu/singularity/tmp"
        # Alphafold specific variables
        - name: ALPHAFOLD_DB
          value: "/storage/brno11-elixir/projects/alphafold/alphafold.db-2.3.1"
        # Default variables
        - name: TMPDIR
          value: "$SCRATCHDIR"
        - name: TMP
          value: "$SCRATCHDIR"
        - name: TEMP
          value: "$SCRATCHDIR"
        # Ensuring a consistent collation environment is good for reproducibility.
        - name: LC_ALL
          value: C
      tpv_dispatcher:
        runner: dynamic
        type: python
        function: map_tool_to_destination
        rules_module: tpv.rules
        tpv_config_files:
          - https://raw.githubusercontent.com/galaxyproject/tpv-shared-database/main/tools.yml
          - "{{ tpv_config_dir }}/tpv_rules_local.yml"
  resources:
    default: default
    groups:
      default: []
      testing: [cores, time]
  tools:
    - class: local # these special tools that aren't parameterized for remote execution - expression tools, upload, etc
      environment: local_env
    - id: testing
      environment: tpv_dispatcher
      resources: testing

galaxy_config:
  galaxy:
    # Main Configuration
    # To not bother the main disk
    new_file_path: "{{ galaxy_mutable_data_dir }}/tmp"
    object_store_config_file: "{{ galaxy_config_dir }}/object_store_conf.xml"
    enable_oidc: true
    oidc_config_file: "{{ galaxy_config_dir }}/oidc_config.xml"
    oidc_backends_config_file: "{{ galaxy_config_dir }}/oidc_backends_config.xml"
    logo_src: "https://www.e-infra.cz/img/logo.svg"
    themes_config_file: "{{ galaxy_config_dir }}/themes.yml"
    admin_users:
    - admin@example.org
    - 499542@muni.cz
    - 325073@mail.muni.cz
    - demko@cesnet.cz
    - ljocha@ics.muni.cz
      #    cleanup_job: never
    smtp_server: "rs.cesnet.cz:25"
    error_email_to: "regalaxy@rt.cesnet.cz"
    allow_user_creation: False
    database_connection: "postgresql:///{{ galaxy_db_name }}?host=/var/run/postgresql"
    file_path: "{{ galaxy_mutable_data_dir }}/datasets"
    job_working_directory: "{{ galaxy_mutable_data_dir }}/jobs"
    object_store_store_by: uuid
    id_secret: "{{ vault_id_secret }}"
    job_config: "{{ galaxy_job_config }}" # Use the variable we defined above
    job_resource_params_file: "{{ galaxy_config_dir }}/job_resource_params_conf.xml"
    sanitize_allowlist_file: "{{ galaxy_mutable_config_dir }}/sanitize_allowlist.txt"
    # SQL Performance
    slow_query_log_threshold: 5
    enable_per_request_sql_debugging: true
    # File serving Performance
    nginx_x_accel_redirect_base: /_x_accel_redirect
    # Automation / Ease of Use / User-facing features
    watch_job_rules: 'auto'
    allow_path_paste: true
    enable_quotas: true
    allow_user_deletion: true
    show_welcome_with_login: true
    expose_user_name: true
    expose_dataset_path: true
    expose_potentially_sensitive_job_metrics: true
    # NFS workarounds
    retry_job_output_collection: 3
    # Debugging
    cleanup_job: onsuccess
    allow_user_impersonation: true
    # Tool security
    outputs_to_working_directory: true
    new_user_dataset_access_role_default_private: true # Make datasets private by default
    # TUS
    galaxy_infrastructure_url: "https://{{ inventory_hostname }}"
    tus_upload_store: "{{ galaxy_tus_upload_store }}"
    # CVMFS
    tool_data_table_config_path: /cvmfs/data.galaxyproject.org/byhand/location/tool_data_table_conf.xml,/cvmfs/data.galaxyproject.org/managed/location/tool_data_table_conf.xml
    # Tool Dependencies
    dependency_resolvers_config_file: "{{ galaxy_config_dir }}/dependency_resolvers_conf.xml"
    containers_resolvers_config_file: "{{ galaxy_config_dir }}/container_resolvers_conf.yml"
    # Data Library Directories
    library_import_dir: /libraries/admin
    user_library_import_dir: /libraries/user
    # Celery
    amqp_internal_connection: "pyamqp://galaxy:{{ vault_rabbitmq_password_galaxy }}@localhost:5671/galaxy_internal?ssl=1"
    celery_conf:
      result_backend: "redis://localhost:6379/0"
    enable_celery_tasks: true
    # Monitoring
    statsd_host: localhost
    statsd_influxdb: true
    # FTP
    ftp_upload_dir: "{{ galaxy_mutable_data_dir }}/uploads"
    ftp_upload_site: "{{ inventory_hostname }}"
  gravity:
    process_manager: systemd
    galaxy_root: "{{ galaxy_root }}/server"
    galaxy_user: "{{ galaxy_user_name }}"
    virtualenv: "{{ galaxy_venv_dir }}"
    gunicorn:
      # listening options
      bind: "unix:{{ galaxy_mutable_config_dir }}/gunicorn.sock"
      # performance options
      workers: 2
      # Other options that will be passed to gunicorn
      # This permits setting of 'secure' headers like REMOTE_USER (and friends)
      # https://docs.gunicorn.org/en/stable/settings.html#forwarded-allow-ips
      extra_args: '--forwarded-allow-ips="*"'
      # This lets Gunicorn start Galaxy completely before forking which is faster.
      # https://docs.gunicorn.org/en/stable/settings.html#preload-app
      preload: true
    celery:
      enable_beat: true
      enable: true
      queues: celery,galaxy.internal,galaxy.external
      pool: threads
      memory_limit: 2
      concurrency: 2
      loglevel: DEBUG
    tusd:
      enable: true
      tusd_path: /usr/local/sbin/tusd
      upload_dir: "{{ galaxy_tus_upload_store }}"
    handlers:
      handler:
        processes: 2
        pools:
          - job-handlers
          - workflow-schedulers
    reports:
      enable: true
      url_prefix: /reports
      bind: "unix:{{ galaxy_mutable_config_dir }}/reports.sock"
      config_file: "{{ galaxy_config_dir }}/reports.yml"

galaxy_job_config_file: "{{ galaxy_config_dir }}/galaxy.yml"

galaxy_config_files_public:
  - src: files/galaxy/welcome.html
    dest: "{{ galaxy_mutable_config_dir }}/welcome.html"
  - src: files/galaxy/usegalaxy-cz.png
    dest: "{{ galaxy_server_dir }}/static/usegalaxy-cz.png"
  - src: files/galaxy/elixir_logo.png
    dest: "{{ galaxy_server_dir }}/static/elixir_logo.png"
  - src: files/galaxy/e-infra_logo.svg
    dest: "{{ galaxy_server_dir }}/static/e-infra_logo.svg"

galaxy_config_files:
  - src: files/galaxy/themes.yml
    dest: "{{ galaxy_config.galaxy.themes_config_file }}"
  - src: "files/{{ inventory_hostname }}/tpv_rules_local.yml"
    dest: "{{ tpv_mutable_dir }}/tpv_rules_local.yml"

galaxy_config_templates:
  - src: templates/galaxy/config/container_resolvers_conf.yml.j2
    dest: "{{ galaxy_config.galaxy.containers_resolvers_config_file }}"
  - src: templates/galaxy/config/dependency_resolvers_conf.xml
    dest: "{{ galaxy_config.galaxy.dependency_resolvers_config_file }}"
  - src: templates/galaxy/config/job_resource_params_conf.xml.j2
    dest: "{{ galaxy_config.galaxy.job_resource_params_file }}"
  - src: templates/galaxy/config/reports.yml
    dest: "{{ galaxy_config.gravity.reports.config_file }}"
  - src: templates/galaxy/config/oidc_config.xml
    dest: "{{ galaxy_config_dir }}/oidc_config.xml"
  - src: "files/{{ inventory_hostname }}/oidc_backends_config.xml"
    dest: "{{ galaxy_config_dir }}/oidc_backends_config.xml"
  - src: "files/{{ inventory_hostname }}/object_store_conf.xml.j2"
    dest: "{{ galaxy_config.galaxy.object_store_config_file }}"
  - src: templates/galaxy/config/tool_conf.xml.j2
    dest: "{{ galaxy_config_dir }}/tool_conf.xml"

galaxy_extra_dirs:
  - "{{ galaxy_mutable_data_dir }}/data"
  - "{{ galaxy_config_dir }}/{{ tpv_config_dir_name }}"

galaxy_extra_privsep_dirs:
  - "{{ tpv_mutable_dir }}"
tpv_privsep: true

galaxy_local_tools:
- testing.xml

# Certbot
certbot_auto_renew_hour: "{{ 23 |random(seed=inventory_hostname)  }}"
certbot_auto_renew_minute: "{{ 59 |random(seed=inventory_hostname)  }}"
certbot_auth_method: --webroot
certbot_install_method: virtualenv
certbot_auto_renew: yes
certbot_auto_renew_user: root
certbot_environment: production
certbot_well_known_root: /srv/nginx/_well-known_root
certbot_share_key_users:
  - www-data
  - proftpd
certbot_share_key_ids:
  - "999:999"
certbot_post_renewal: |
    systemctl restart nginx || true
    docker restart rabbit_hole || true
    systemctl restart proftpd || true
certbot_domains:
 - "{{ inventory_hostname }}"
certbot_agree_tos: --agree-tos

# NGINX
nginx_selinux_allow_local_connections: true
nginx_servers:
  - redirect-ssl
nginx_ssl_servers:
  - galaxy
    #  - sentry
nginx_enable_default_server: false
nginx_conf_http:
  client_max_body_size: 1g
  # gzip: "on" # This is enabled by default in Ubuntu, and the duplicate directive will cause a crash.
  gzip_proxied: "any"
  gzip_static: "on"   # The ngx_http_gzip_static_module module allows sending precompressed files with the ".gz" filename extension instead of regular files.
  gzip_vary: "on"
  gzip_min_length: 128
  gzip_comp_level: 6  # Tradeoff of better compression for slightly more CPU time.
  gzip_types: |
      text/plain
      text/css
      text/xml
      text/javascript
      application/javascript
      application/x-javascript
      application/json
      application/xml
      application/xml+rss
      application/xhtml+xml
      application/x-font-ttf
      application/x-font-opentype
      image/png
      image/svg+xml
      image/x-icon

# default Let's encrypt, override in host_vars eventually
#nginx_ssl_role: usegalaxy_eu.certbot
#nginx_conf_ssl_certificate: /etc/letsencrypt/live/{{ inventory_hostname }}/fullchain.pem
#nginx_conf_ssl_certificate_key: /etc/letsencrypt/live/{{ inventory_hostname }}/privkey.pem

#Install pip docker package for ansible
pip_install_packages:
  - name: docker
# RabbitMQ
rabbitmq_hostname: "{{ inventory_hostname }}"
rabbitmq_container:
  name: rabbit_hole
  image: rabbitmq:3.13-management
  hostname: "{{ inventory_hostname }}"

rabbitmq_plugins:
  - rabbitmq_management

rabbitmq_conf_ssl_certificate: /etc/ssl/certs/cert.pem
rabbitmq_conf_ssl_certificate_key: /etc/ssl/user/privkey-999:999.pem

rabbitmq_config:
  listeners:
    tcp: none
  ssl_listeners:
    default: 5671
  ssl_options:
    verify: verify_peer
    cacertfile: /etc/ssl/certs/ca-certificates.crt
    certfile: "{{ rabbitmq_conf_ssl_certificate }}"
    keyfile: "{{ rabbitmq_conf_ssl_certificate_key }}"
    fail_if_no_peer_cert: 'false'
    versions:
      - tlsv1.3
      - tlsv1.2
  management_agent:
    disable_metrics_collector: "false"
  management:
    disable_stats: 'false'
  consumer_timeout: 21600000 # 6 hours in milliseconds

rabbitmq_vhosts:
  - pulsar
  - galaxy_gpu
  - galaxy_internal

rabbitmq_users:
  - user: debian
    password: "{{ rabbitmq_users_password.mqadmin }}"
    tags: administrator
    vhost: /
  - user: pulsar
    password: "{{ rabbitmq_users_password.pulsar }}"
    vhost: pulsar
  - user: galaxy
    password: "{{ vault_rabbitmq_password_galaxy }}"
    vhost: galaxy_internal
  - user: flower
    password: "{{ vault_rabbitmq_password_flower }}"
    tags: administrator
    vhost: galaxy_internal

# TUS
galaxy_tusd_port: 1080
galaxy_tus_upload_store: "{{ galaxy_mutable_data_dir }}/tus" # /mnt/data/tus

#Redis
galaxy_additional_venv_packages:
  - redis

# Flower
flower_python_version: python3
flower_app_dir: "{{ galaxy_root }}"
flower_python_path: "{{ galaxy_root }}/server/lib"
flower_venv_dir: "{{ galaxy_venv_dir }}"
flower_app_name: galaxy.celery
flower_db_file: "{{ galaxy_root }}/var/flower.db"
flower_persistent: true
flower_broker_api: "https://flower:{{ vault_rabbitmq_password_flower }}@localhost:5671/api/"
flower_broker_url: "amqp://flower:{{ vault_rabbitmq_password_flower }}@localhost:5671/galaxy_internal?ssl=true"
flower_proxy_prefix: /flower

flower_ui_users:
  - name: admin
    password: "{{ vault_flower_user_password}}"

flower_environment_variables:
  GALAXY_CONFIG_FILE: "{{ galaxy_config_file }}"

# Telegraf
telegraf_plugins_extra:
  listen_galaxy_routes:
    plugin: "statsd"
    config:
      - service_address = ":8125"
      - metric_separator = "."
      - allowed_pending_messages = 10000
  monitor_galaxy_queue:
    plugin: "exec"
    config:
      - commands = ["/usr/bin/env PGDATABASE=galaxy /usr/local/bin/gxadmin iquery queue-overview --short-tool-id"]
      - timeout = "10s"
      - data_format = "influx"
      - interval = "15s"

# TIaaS setup
tiaas_dir: /srv/tiaas
tiaas_admin_user: admin
tiaas_admin_pass: changeme

# Proftpd:
proftpd_galaxy_auth: yes
galaxy_ftp_upload_dir: "{{ galaxy_config.galaxy.ftp_upload_dir }}"
proftpd_display_connect: |
  {{ inventory_hostname }} FTP server

  Unauthorized access is prohibited
proftpd_create_ftp_upload_dir: yes
proftpd_options:
  - User: galaxy
  - Group: galaxy
  - Port: 21
proftpd_sql_db: galaxy@/var/run/postgresql
proftpd_sql_user: galaxy
proftpd_conf_ssl_certificate: /etc/letsencrypt/live/{{ inventory_hostname }}/cert.pem
proftpd_conf_ssl_certificate_key: /etc/letsencrypt/live/{{ inventory_hostname }}/privkey.pem
proftpd_global_options:
  - PassivePorts: 56000 60000
proftpd_use_mod_tls_shmcache: false
proftpd_tls_options: NoSessionReuseRequired

