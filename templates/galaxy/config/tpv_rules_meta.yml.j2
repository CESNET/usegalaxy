global:
  default_inherits: default

tools:
  default:
    abstract: True
    cores: 1
    mem: "int(cores * 4)"
    env:
      # Ensuring a consistent collation environment is good for reproducibility.
    - name: LC_ALL
      value: C
    - name: GALAXY_SLOTS
      value: "{int(cores)}"
    - name:  GALAXY_MEMORY_MB
      value: "{int(mem)*1000}"
    context:
      walltime: 24
      scratch: 50
      queue: "{{ pulsar.pbs_queue }}"
    scheduling:
      require:
        - pulsar
      reject:
        - offline
    rules: []
    rank: |
      helpers.weighted_random_sampling(candidate_destinations)

  local_running_tools:
    abstract: True
    scheduling:
      require:
      - local
      reject:
      - pulsar

  local_sing_running_tools:
    inherits: local_running_tools
    abstract: True
    scheduling:
      require:
      - singularity

  testing_rosettafold2:
    cores: 8
    mem: 64
    gpus: 1
    context:
      walltime: 24
      scratch: 25
      gpu_mem: 16
      queue: "{{ pulsar.pbs_gpu_queue }}"
    scheduling:
      require:
        - rosettafold

  rosettafold2:
    inherits: testing_rosettafold2
    scheduling:
      require:
        - singularity

  testing.*:
    cores: 1
    mem: 1
    context:
      walltime: 1
    rules:
      - id: admin_only_testing_tool
        if: |
          # Only allow the tool to be executed if the user is an admin
          admin_users = app.config.admin_users
          # last line in block must evaluate to a value - which determines whether the TPV if conditional matches or not
          not user or (user.email not in admin_users and user.email not in ['galaxy@cesnet.cz','galaxy@usegalaxy.cz','galaxyelixir@repeatexplorer-elixir.cerit-sc.cz'])
        fail: Unauthorized. Only admins can execute this tool.

      - id: resource_params_defined
        if: |
          param_dict = job.get_param_values(app)
          param_dict.get('__job_resource', {}).get('__job_resource__select') == 'yes'
        cores: int(job.get_param_values(app)['__job_resource']['cores'])
        context:
           walltime: "{int(job.get_param_values(app)['__job_resource']['time'])}"

  .*/alphafold/.*:
    cores: 8
    mem: 120
    gpus: 1
    context:
      walltime: 24
      scratch: 100
      gpu_mem: 16
      queue: "{{ pulsar.pbs_gpu_queue }}"
    env:
      MPLCONFIGDIR: "$SCRATCHDIR"
      ALPHAFOLD_DB: "/scratch.ssd/galaxyeu/permanent/alphafold.db"
      ALPHAFOLD_USE_GPU: True
    scheduling:
      require:
        - alphafold

  .*/goenrichment/.*:
    scheduling:
      require:
        - nasty-java

  .*/bedtools_annotatebed/.*:
    context:
      walltime: 48

  .*/megahit/.*:
    cores: 16
    mem: 256
    context:
      walltime: 96
      scratch: 100
      queue: elixircz
    scheduling:
      require:
        - highmem

  .*/fastq_groomer/.*:
    cores: 1
    mem: 16

  .*/hapog/.*:
    cores: 8
    context:
      walltime: 48

  __DATA_FETCH__:
    inherits: local_running_tools

  __IMPORT_HISTORY__:
    inherits: local_running_tools

  __EXPORT_HISTORY__:
    inherits: local_running_tools

  __SET_.*:
    inherits: local_running_tools

  "Show .*":
    inherits: local_running_tools

  "Remove .*":
    inherits: local_running_tools

  cat1:
    inherits: local_running_tools

  export_remote:
    inherits: local_running_tools

  gff2bed1:
    inherits: local_running_tools

  .*/data_manager_.*:
    inherits: local_sing_running_tools

  ebi_sra_main:
    inherits: local_sing_running_tools

  sra_source:
    inherits: local_sing_running_tools

#  bed2gff1:
#    inherits: local_sing_running_tools

  Extract_features1:
    inherits: local_sing_running_tools


roles:
  training.*:
    max_cores: 4
    max_mem: "int(max_cores * 3.8)"
    scheduling:
      require:
        - training

  # DEMON: For testing purposes, set whatever you want here when needed
  test.*:
    scheduling:
      require:
        - test

  highmem:
    mem: 512
    context:
      queue: elixircz
    scheduling:
      require:
        - highmem
    

destinations:
  tpv_local:
    runner: local_runner
    max_cores: 1
    max_mem: 4
    params:
      tmp_dir: true
    scheduling:
      require:
        - local
      reject:
        - singularity
  tpv_local_singularity:
    runner: local_runner
    max_cores: 1
    max_mem: 4
    params:
      require_container: true
      singularity_enabled: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.8.3"
    env:
      # The cache directory holds the docker containers that get converted
      SINGULARITY_CACHEDIR: "/cvmfs/singularity.galaxyproject.org/all/"
      # Singularity uses a temporary directory to build the squashfs filesystem
      SINGULARITY_TMPDIR: "{{ galaxy_mutable_data_dir }}/tmp"
    scheduling:
      require:
      - local
      - singularity
  # DEMON: this is the common pulsar configuration
  tpv_pulsar_common:
    abstract: true
    runner: pulsar_tpv_runner
    # DEMON: sum of cores in one VM (i.e., elmo5-*)
    max_accepted_cores: 32
    # DEMON: sum of memory in one VM (i.e., elmo5-*)
    max_accepted_mem: 185
    # DEMON: sum of GPUs in one VM (i.e., elmo5-*)
    max_accepted_gpus: 0
    # DEMON: max number of cores one job will get
    max_cores: 32
    # DEMON: max memory one job will get
    max_mem: 180
    # DEMON: max number of gpus one job will get
    max_gpus: 0
    params:
      default_file_action: remote_rsync_transfer
      dependency_resolution: remote
      jobs_directory: "{{ pulsar_data_dir }}/files/staging"
      persistence_directory: "/opt/pulsar/files/persistent"
      remote_metadata: false
      rewrite_parameters: true
      transport: rsync
      ssh_user: "{{ galaxy_user_name }}"
      ssh_host: "{{ inventory_hostname }}"
      ssh_port: 22
      ssh_key: |
        {{ pulsar_ssh_key | indent(width=8,first=False) }}
      outputs_to_working_directory: false
      submit_native_specification: "-l select=1:ncpus={int(cores)}:mem={int(mem)}gb:scratch_local={int(scratch)}gb -l walltime={int(walltime)}:00:00 -q {queue} -N {{ pulsar.nfs_prefix }}_j{job.id}__{tool.id if '/' not in tool.id else tool.id.split('/')[-2]+'_v'+tool.id.split('/')[-1]}__{user.username if user and hasattr(user, 'username') else 'anonymous'}"
    env:
      TMPDIR: $SCRATCHDIR
      TMP: $SCRATCHDIR
      TEMP: $SCRATCHDIR
    scheduling:
      require:
        - pulsar  
  # DEMON: this one uses conda dependences (no singularity)
  tpv_pulsar_conda:
    inherits: tpv_pulsar_common
    scheduling:
      require:
        - conda
      reject:
        - singularity
  # DEMON: this one uses singularity exclusively (consider it as a default configuration)
  tpv_pulsar:
    inherits: tpv_pulsar_common
    runner: pulsar_tpv_runner
    params:
      singularity_enabled: true
      singularity_volumes: "$defaults,/cvmfs/data.galaxyproject.org:ro,$SCRATCHDIR:rw"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.8.3"
      singularity_run_extra_arguments: >-
        --env JAVA_OPTS="-Xmx{int(mem)}g -Djava.io.tmpdir=$SCRATCHDIR"
        --env JAVA_TOOL_OPTIONS="-Xmx{int(mem)}g -Djava.io.tmpdir=$SCRATCHDIR"
    env:
      SINGULARITY_CACHEDIR: "/cvmfs/singularity.galaxyproject.org/all/"
      SINGULARITY_TMPDIR: $SCRATCHDIR
      XDG_CACHE_HOME: $SCRATCHDIR
    scheduling:
      prefer:
        - singularity
  # DEMON: Some JAVA tools just ignore the traditional way of limiting the allowed memory
  tpv_pulsar_nasty_java:
    inherits: tpv_pulsar
    runner: pulsar_tpv_runner
    params:
      singularity_run_extra_arguments: '--env _JAVA_OPTIONS="-Xmx{int(mem)}g -Djava.io.tmpdir=$SCRATCHDIR"'
    scheduling:
      require:
        - nasty-java
  # DEMON: Pulsar for AlphaFold because of special configuration
  tpv_pulsar_alphafold:
    inherits: tpv_pulsar
    runner: pulsar_tpv_runner
    max_accepted_gpus: 1
    max_gpus: 1
    params:
      singularity_run_extra_arguments: '--nv --env SCRATCHDIR="$SCRATCHDIR" --env SCRATCH=$SCRATCHDIR'
      singularity_volumes: '$defaults,$SCRATCHDIR,$ALPHAFOLD_DB:/data/2.3:ro'
      submit_native_specification: "-l select=1:ncpus={int(cores)}:mem={int(mem)}gb:scratch_local={int(scratch)}gb:ngpus={int(gpus)}:gpu_mem={int(gpu_mem)}gb -l walltime={int(walltime)}:00:00 -q {queue} -N {{ pulsar.nfs_prefix }}_j{job.id}__{tool.id if '/' not in tool.id else tool.id.split('/')[-2]+'_v'+tool.id.split('/')[-1]}__{user.username if user and hasattr(user, 'username') else 'anonymous'}"
    scheduling:
      require:
        - alphafold
  # DEMON: Pulsar for TIAAS. It could schedule to a private small queue to ensure the participants of some course will not have to wait
  tpv_pulsar_training:
    inherits: tpv_pulsar
    runner: pulsar_tpv_runner
    scheduling:
      require:
        - training
  # DEMON: For tools/users that need more memory for their jobs. Current approach is to add the user to highmem role for some time. Better approach would be to implement rescheduling for PBS over Pulsar runner
  tpv_pulsar_highmem:
    inherits: tpv_pulsar
    runner: pulsar_tpv_runner
    max_accepted_mem: 512
    max_mem: 512
    scheduling:
      require:
        - highmem
  # DEMON: For testing purposes, set whatever you want here when needed
  tpv_pulsar_test:
    inherits: tpv_pulsar
    runner: pulsar_tpv_runner
    scheduling:
      require:
        - test
# DEMON: usegalaxy.cz specific destinations (unfortunately, because of inheritance they cannot be in placed in instance-specific TPV config file)
  tpv_pulsar_rosettafold:
    inherits: tpv_pulsar
    runner: pulsar_tpv_runner
    params:
      singularity_enabled: false
      submit_native_specification: "-l select=1:ncpus={int(cores)}:mem={int(mem)}gb:scratch_local={int(scratch)}gb:ngpus={int(gpus)}:gpu_mem={int(gpu_mem)}gb -l walltime={int(walltime)}:00:00 -q {queue} -N pulsar_cz_j{job.id}__{tool.id if '/' not in tool.id else tool.id.split('/')[-2]+'_v'+tool.id.split('/')[-1]}__{user.username if user and hasattr(user, 'username') else 'anonymous'}"
    scheduling:
      require:
        - rosettafold
  tpv_pulsar_rosettafold_sing:
    inherits: tpv_pulsar_rosettafold
    container_resolvers:
    -   type: explicit_singularity
    -   cache_directory: /cvmfs/singularity.metacentrum.cz/RoseTTAFold2/
        type: cached_mulled_singularity
    params:
      singularity_enabled: true
      singularity_run_extra_arguments: '--nv --env SCRATCHDIR="$SCRATCHDIR" --env SCRATCH=$SCRATCHDIR'
      singularity_volumes: '$job_directory:ro,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,$SCRATCHDIR,/scratch.ssd/galaxyeu/permanent/rosettafold_data/bfd:/opt/RoseTTAFold2/bfd,/scratch.ssd/galaxyeu/permanent/rosettafold_data/pdb100_2021Mar03:/opt/RoseTTAFold2/pdb100_2021Mar03,/scratch.ssd/galaxyeu/permanent/rosettafold_data/UniRef30_2020_06:/opt/RoseTTAFold2/UniRef30_2020_06,/scratch.ssd/galaxyeu/permanent/rosettafold_data/weights:/opt/RoseTTAFold2/network/weights'
      singularity_default_container_id: "/cvmfs/singularity.metacentrum.cz/RoseTTAFold2/rosettafold_image.sif"
    scheduling:
      require:
        - singularity
